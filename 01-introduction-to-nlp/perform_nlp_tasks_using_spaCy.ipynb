{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:applied_nlp_base_mac] *",
      "language": "python",
      "name": "conda-env-applied_nlp_base_mac-py"
    },
    "colab": {
      "name": "perform-nlp-tasks-using-spaCy.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/applied-nlp-in-enterprise/blob/main/01-introduction-to-nlp/perform_nlp_tasks_using_spaCy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PumcNuWd5-M"
      },
      "source": [
        "## Perform NLP Tasks using SpaCy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ttd5Lxw0d5-M"
      },
      "source": [
        "Let’s now use SpaCy for our NLP tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y9XXXT_d5-M"
      },
      "source": [
        "First, install spaCy. For more resources on how to install spaCy, please visit the [official SpaCy website](https://spacy.io/usage)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2OdL4NUd5-M"
      },
      "source": [
        "If you haven't installed spaCy already, these commands will get you everything you need. If you're running them in a notebook, prefix each line with a `!` character, as we've done before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHUQ5jD5g6qi"
      },
      "source": [
        "%%shell\n",
        "\n",
        "pip install -U spacy[cuda110,transformers,lookups]==3.0.3\n",
        "pip install -U spacy-lookups-data==1.0.0\n",
        "pip install cupy-cuda110==8.5.0\n",
        "python -m spacy download en_core_web_trf\n",
        "python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47nK3sUqm1ZH"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload() # upload kaggle.json file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7L5iIEj4m2Ba",
        "outputId": "b1db7cce-30b7-433e-c211-273c40df5c9b"
      },
      "source": [
        "%%shell\n",
        "\n",
        "mkdir -p ~/.kaggle\n",
        "mv kaggle.json ~/.kaggle/\n",
        "ls ~/.kaggle\n",
        "chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "# download word embeddings from kaggle\n",
        "kaggle datasets download -d tunguz/200000-jeopardy-questions\n",
        "unzip -qq 200000-jeopardy-questions.zip\n",
        "rm -rf 200000-jeopardy-questions.zip"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kaggle.json\n",
            "Downloading 200000-jeopardy-questions.zip to /content\n",
            " 70% 8.00M/11.5M [00:00<00:00, 81.1MB/s]\n",
            "100% 11.5M/11.5M [00:00<00:00, 73.3MB/s]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QweG2Or2d5-M"
      },
      "source": [
        "## SpaCy Pretrained Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozB2EqNXd5-M"
      },
      "source": [
        "SpaCy has pretrained language models for out-of-the-box use. Pretrained models are models that have been trained on lots of data already and are ready for us to perform inference with.\n",
        "\n",
        "These pretrained language models will help us solve the basic NLP tasks, but more advanced users are welcome to fine-tune the pretrained models on more specific data of their choosing. This will deliver even better performance for their specific tasks at hand.\n",
        "\n",
        "Fine-tuning is the process of taking a pretrained model and training it some more (i.e., fine-tuning the model) on a more specific corpus of text that is relevant to the domain of the user.footnote:[This operation of taking a model developed for one task and using it as a starting point for a model on a second task is known as transfer learning.] \n",
        "\n",
        "For example, if we worked in finance, we may decide to fine-tune a generic pretrained language model on financial documents to generate a finance-specific language model. This finance-specific language model would have even better performance on finance-related NLP tasks versus the generic pretrained language model.\n",
        "\n",
        "SpaCy breaks out its pretrained language models into two groups: \n",
        "\n",
        "- core models \n",
        "- starter models\n",
        "\n",
        "The core models are general-purpose models and will help us solve the basic NLP tasks. \n",
        "\n",
        "The starter models are base models useful for transfer learning; these models have pretrained weights which you could use to initialize and fine-tune for your own models. \n",
        "\n",
        "Think of the core models as ready-to-go models and the base models as do-it-yourself starter kits.\n",
        "\n",
        "We will use the ready-to-go core models to perform the basic NLP tasks. \n",
        "\n",
        "Let's first import the core model footnote:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TzPGPe4d5-N"
      },
      "source": [
        "# Import spacy and download language model\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK18_5nwd5-N"
      },
      "source": [
        "Now, let’s perform the first of the NLP tasks: tokenization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORvnFZwFd5-N"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWmblhCZd5-N"
      },
      "source": [
        "Tokenization is where all NLP work begins; before the machine can process any of the text it sees, it must break the text into bite-size tokens. Tokenization will segment text into words, punctuation marks, etc.\n",
        "\n",
        "SpaCy automatically runs the entire NLP pipeline when you run a language model on the data (i.e., `nlp(SENTENCE)`), but to isolate just the tokenizer, we will invoke just the tokenizer using `nlp.tokenizer(SENTENCE)`.\n",
        "\n",
        "Then, we will print the length of the tokens and the individual tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nKLk3DJd5-N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7b209f0-1898-489e-aea7-813dd5118952"
      },
      "source": [
        "# Tokenization\n",
        "sentence = nlp.tokenizer(\"We live in Paris.\")\n",
        "\n",
        "# Length of sentence\n",
        "print(\"The number of tokens: \", len(sentence))\n",
        "\n",
        "# Print individual words (i.e., tokens)\n",
        "print(\"The tokens: \")\n",
        "for words in sentence:\n",
        "    print(words)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of tokens:  5\n",
            "The tokens: \n",
            "We\n",
            "live\n",
            "in\n",
            "Paris\n",
            ".\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xq-XNsTgd5-O"
      },
      "source": [
        "The length of tokens is 5, and the individual tokens are `\"We\"`, `\"live\"`, `\"in\"`, `\"Paris\"`, `\".\"`. The period at the end of the sentence is its own token.\n",
        "\n",
        "Note that the spaCy tokenizer will treat new lines (`\"\\n\"`), tabs (`\"\\t\"`), and whitespace characters beyond a single space (`\" \"`) as tokens.\n",
        "\n",
        "Let's try the tokenizer on a slightly more complex example.\n",
        "\n",
        "We will load in publicly available Jeopardy Questions and then run the entire SpaCy language model on a few of the questions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohWmxf9gd5-O"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "cwd = os.getcwd()\n",
        "\n",
        "# Import Jeopardy Questions\n",
        "data = pd.read_csv(\"JEOPARDY_CSV.csv\")\n",
        "data = pd.DataFrame(data=data)\n",
        "\n",
        "# Lowercase, strip whitespace, and view column names\n",
        "data.columns = map(lambda x: x.lower().strip(), data.columns)\n",
        "\n",
        "# Reduce size of data\n",
        "data = data[0:1000] \n",
        "\n",
        "# Tokenize Jeopardy Questions\n",
        "data[\"question_tokens\"] = data[\"question\"].apply(lambda x: nlp(x))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ffhffGkd5-O"
      },
      "source": [
        "For the first 1,000 Jeopardy questions, we have now created tokens. In other words, you have created tokens for each and every single one of the 1,000 Jeopardy questions.\n",
        "\n",
        "To make sure everything worked right, let’s view the first question and the tokens created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff1jBi4bd5-O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12d54a9f-a721-41be-c2e7-fc23a68f790e"
      },
      "source": [
        "# View first question\n",
        "example_question = data.question[0]\n",
        "example_question_tokens = data.question_tokens[0]\n",
        "print(\"The first questions is:\")\n",
        "print(example_question)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The first questions is:\n",
            "For the last 8 years of his life, Galileo was under house arrest for espousing this man's theory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHUEhRO2d5-P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fe456b1-132a-429b-8ed1-11d0e2b478df"
      },
      "source": [
        "# Print individual tokens of first question\n",
        "print(\"The tokens from the first question are:\")\n",
        "for tokens in example_question_tokens:\n",
        "    print(tokens)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tokens from the first question are:\n",
            "For\n",
            "the\n",
            "last\n",
            "8\n",
            "years\n",
            "of\n",
            "his\n",
            "life\n",
            ",\n",
            "Galileo\n",
            "was\n",
            "under\n",
            "house\n",
            "arrest\n",
            "for\n",
            "espousing\n",
            "this\n",
            "man\n",
            "'s\n",
            "theory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFusQNfQd5-P"
      },
      "source": [
        "This is the first basic NLP task machines perform; now we can move onto the other NLP tasks. Well done!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXjwUOEzd5-P"
      },
      "source": [
        "## Part-of-speech Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZhT6FBFd5-P"
      },
      "source": [
        "After tokenization, machines need to tag each token with relevant metadata such as the part-of-speech of each token. This is what we will perform now.\n",
        "\n",
        "Since we applied the entire SpaCy language model to the Jeopardy questions, the tokens generated already have a lot of the meaningful attributes/metadata we care about.\n",
        "\n",
        "SpaCy uses pre-loaded statistical models to predict the part-of-speech of each token. We loaded the English language statistical model earlier using the following code: `spacy.load(\"en_core_web_sm\")`.\n",
        "\n",
        "Let's take a look at the Part-of-speech (POS) Tagging attributes for the tokens in the first question."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIDr7WBOd5-P",
        "outputId": "d6222888-5b00-4bd1-8d54-5850cc0960d2"
      },
      "source": [
        "# Print Part-of-speech tags for tokens in the first question\n",
        "print(\"Here are the Part-of-speech tags for each token in the first question:\")\n",
        "for token in example_question_tokens:\n",
        "    print(token.text,token.pos_, spacy.explain(token.pos_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Here are the Part-of-speech tags for each token in the first question:\n",
            "For ADP adposition\n",
            "the DET determiner\n",
            "last ADJ adjective\n",
            "8 NUM numeral\n",
            "years NOUN noun\n",
            "of ADP adposition\n",
            "his PRON pronoun\n",
            "life NOUN noun\n",
            ", PUNCT punctuation\n",
            "Galileo PROPN proper noun\n",
            "was AUX auxiliary\n",
            "under ADP adposition\n",
            "house NOUN noun\n",
            "arrest NOUN noun\n",
            "for ADP adposition\n",
            "espousing VERB verb\n",
            "this DET determiner\n",
            "man NOUN noun\n",
            "'s PART particle\n",
            "theory NOUN noun\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGnpqmNld5-Q"
      },
      "source": [
        "The first token \"For\" is marked as an adposition (e.g., in, to, during), the second token \"the\" is a determiner (e.g., a, an, the), the third token \"last\" is an adjective, the fourth token \"8\" is a numeral, the fifth token \"years\" is a noun, and so on.\n",
        "\n",
        "Figure 1-2 displays the full list of all possible POS tags, including descriptions and examples of each.footnote:[Please visit the [SpaCy POS documentation](https://spacy.io/api/annotation) for more.]\n",
        "\n",
        "![Part-of-speech Tags](https://github.com/nlpbook/nlpbook/blob/main/images/hulp_0102.png?raw=1)\n",
        "\n",
        "Now that we have used the tokenizer to create tokens for each sentence and part-of-speech tagging to tag each token with meaningful attributes, let's label each token's relationship with other tokens in the sentence. In other words, let's find the inherent structure among the tokens given the part-of-speech metadata we have generated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRh8DaoZd5-Q"
      },
      "source": [
        "#### Dependency Parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZM3Ew86jd5-Q"
      },
      "source": [
        "Dependency parsing is the process to find these relationships among the tokens. Once we have performed this step, we will be able to visualize the relationships using a dependency parsing graph.\n",
        "\n",
        "First, let's view the depenency parsing tags for each of the tokens in the first question."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_l1zN_0hd5-Q",
        "outputId": "c2f1393e-39f2-4f61-e501-ece2e4ac2f54"
      },
      "source": [
        "# Print Dependency Parsing tags for tokens in the first question\n",
        "for token in example_question_tokens:\n",
        "    print(token.text,token.dep_, spacy.explain(token.dep_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For prep prepositional modifier\n",
            "the det determiner\n",
            "last amod adjectival modifier\n",
            "8 nummod numeric modifier\n",
            "years pobj object of preposition\n",
            "of prep prepositional modifier\n",
            "his poss possession modifier\n",
            "life pobj object of preposition\n",
            ", punct punctuation\n",
            "Galileo nsubj nominal subject\n",
            "was ROOT None\n",
            "under prep prepositional modifier\n",
            "house compound compound\n",
            "arrest pobj object of preposition\n",
            "for prep prepositional modifier\n",
            "espousing pcomp complement of preposition\n",
            "this det determiner\n",
            "man poss possession modifier\n",
            "'s case case marking\n",
            "theory dobj direct object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzp1kJcUd5-Q"
      },
      "source": [
        "The first token \"For\" is marked as a prepositional modifier, the second token \"the\" is a determiner, the third token \"last\" is an adjectival modifier, the fourth token \"8\" is a numeric modifier, the fifth token \"years\" is the object of preposition, and so on.\n",
        "\n",
        "Figures 1-3 and 1-4 list all the possible syntactic dependency tags, including descriptions and examples of each.footnote:[Please visit the [SpaCy documentation](https://spacy.io/api/annotation) for more.]\n",
        "\n",
        "![Syntactic Dependency Parsing Labels Part 1](https://github.com/nlpbook/nlpbook/blob/main/images/hulp_0103.png?raw=1)\n",
        "\n",
        "![Syntactic Dependency Parsing Labels Part 2](https://github.com/nlpbook/nlpbook/blob/main/images/hulp_0104.png?raw=1)\n",
        "\n",
        "These tags help define the relationships among the tokens; using these tags, we can understand the relationship structure among the tokens that make up the sentence.\n",
        "\n",
        "Dependency parsing is hard to unpack so let’s use spaCy’s built-in visualizer to get a better sense of the dependencies across the tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVWyHwmmd5-R",
        "outputId": "2d48fb99-f426-4636-a3ef-4ecbd0c4f5dc"
      },
      "source": [
        "# Visualize the dependency parse\n",
        "from spacy import displacy\n",
        "\n",
        "displacy.render(example_question_tokens, style='dep',\n",
        "                jupyter=True, options={'distance': 120})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"80276e7c4fd14bad8d4853ff662bb401-0\" class=\"displacy\" width=\"2330\" height=\"437.0\" direction=\"ltr\" style=\"max-width: none; height: 437.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">For</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"170\">the</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"170\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"290\">last</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"290\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">8</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">NUM</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"530\">years</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"530\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">of</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">his</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"890\">life,</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"890\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1010\">Galileo</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1010\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1130\">was</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1130\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1250\">under</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1250\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1370\">house</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1370\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1490\">arrest</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1490\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1610\">for</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1610\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1730\">espousing</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1730\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1850\">this</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1850\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1970\">man</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1970\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2090\">'s</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2090\">PART</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2210\">theory</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2210\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-80276e7c4fd14bad8d4853ff662bb401-0-0\" stroke-width=\"2px\" d=\"M70,302.0 C70,2.0 1130.0,2.0 1130.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-80276e7c4fd14bad8d4853ff662bb401-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,304.0 L62,292.0 78,292.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-80276e7c4fd14bad8d4853ff662bb401-0-1\" stroke-width=\"2px\" d=\"M190,302.0 C190,122.0 520.0,122.0 520.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-80276e7c4fd14bad8d4853ff662bb401-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M190,304.0 L182,292.0 198,292.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-80276e7c4fd14bad8d4853ff662bb401-0-2\" stroke-width=\"2px\" d=\"M310,302.0 C310,182.0 515.0,182.0 515.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-80276e7c4fd14bad8d4853ff662bb401-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M310,304.0 L302,292.0 318,292.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-80276e7c4fd14bad8d4853ff662bb401-0-3\" stroke-width=\"2px\" d=\"M430,302.0 C430,242.0 510.0,242.0 510.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-80276e7c4fd14bad8d4853ff662bb401-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M430,304.0 L422,292.0 438,292.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-80276e7c4fd14bad8d4853ff662bb401-0-4\" stroke-width=\"2px\" d=\"M70,302.0 C70,62.0 525.0,62.0 525.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-80276e7c4fd14bad8d4853ff662bb401-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M525.0,304.0 L533.0,292.0 517.0,292.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-80276e7c4fd14bad8d4853ff662bb401-0-5\" stroke-width=\"2px\" d=\"M550,302.0 C550,242.0 630.0,242.0 630.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-80276e7c4fd14bad8d4853ff662bb401-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M630.0,304.0 L638.0,292.0 622.0,292.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-80276e7c4fd14bad8d4853ff662bb401-0-6\" stroke-width=\"2px\" d=\"M790,302.0 C790,242.0 870.0,242.0 870.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-80276e7c4fd14bad8d4853ff662bb401-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M790,304.0 L782,292.0 798,292.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-80276e7c4fd14bad8d4853ff662bb401-0-7\" stroke-width=\"2px\" d=\"M670,302.0 C670,182.0 875.0,182.0 875.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-80276e7c4fd14bad8d4853ff662bb401-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M875.0,304.0 L883.0,292.0 867.0,292.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-80276e7c4fd14bad8d4853ff662bb401-0-8\" stroke-width=\"2px\" d=\"M1030,302.0 C1030,242.0 1110.0,242.0 1110.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-80276e7c4fd14bad8d4853ff662bb401-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1030,304.0 L1022,292.0 1038,292.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-80276e7c4fd14bad8d4853ff662bb401-0-9\" stroke-width=\"2px\" d=\"M1150,302.0 C1150,242.0 1230.0,242.0 1230.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-80276e7c4fd14bad8d4853ff662bb401-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1230.0,304.0 L1238.0,292.0 1222.0,292.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-80276e7c4fd14bad8d4853ff662bb401-0-10\" stroke-width=\"2px\" d=\"M1390,302.0 C1390,242.0 1470.0,242.0 1470.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-80276e7c4fd14bad8d4853ff662bb401-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1390,304.0 L1382,292.0 1398,292.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-80276e7c4fd14bad8d4853ff662bb401-0-11\" stroke-width=\"2px\" d=\"M1270,302.0 C1270,182.0 1475.0,182.0 1475.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-80276e7c4fd14bad8d4853ff662bb401-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1475.0,304.0 L1483.0,292.0 1467.0,292.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-80276e7c4fd14bad8d4853ff662bb401-0-12\" stroke-width=\"2px\" d=\"M1510,302.0 C1510,242.0 1590.0,242.0 1590.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-80276e7c4fd14bad8d4853ff662bb401-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1590.0,304.0 L1598.0,292.0 1582.0,292.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-80276e7c4fd14bad8d4853ff662bb401-0-13\" stroke-width=\"2px\" d=\"M1630,302.0 C1630,242.0 1710.0,242.0 1710.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-80276e7c4fd14bad8d4853ff662bb401-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pcomp</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1710.0,304.0 L1718.0,292.0 1702.0,292.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-80276e7c4fd14bad8d4853ff662bb401-0-14\" stroke-width=\"2px\" d=\"M1870,302.0 C1870,242.0 1950.0,242.0 1950.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-80276e7c4fd14bad8d4853ff662bb401-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1870,304.0 L1862,292.0 1878,292.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-80276e7c4fd14bad8d4853ff662bb401-0-15\" stroke-width=\"2px\" d=\"M1990,302.0 C1990,182.0 2195.0,182.0 2195.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-80276e7c4fd14bad8d4853ff662bb401-0-15\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1990,304.0 L1982,292.0 1998,292.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-80276e7c4fd14bad8d4853ff662bb401-0-16\" stroke-width=\"2px\" d=\"M1990,302.0 C1990,242.0 2070.0,242.0 2070.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-80276e7c4fd14bad8d4853ff662bb401-0-16\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2070.0,304.0 L2078.0,292.0 2062.0,292.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-80276e7c4fd14bad8d4853ff662bb401-0-17\" stroke-width=\"2px\" d=\"M1750,302.0 C1750,62.0 2205.0,62.0 2205.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-80276e7c4fd14bad8d4853ff662bb401-0-17\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2205.0,304.0 L2213.0,292.0 2197.0,292.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4UZ9TXrd5-R"
      },
      "source": [
        "Figure 1-5 displays the first part of the sentence parsed.\n",
        "\n",
        "![Dependency Parsing Example - Part 1](https://github.com/nlpbook/nlpbook/blob/main/images/hulp_0105.png?raw=1)\n",
        "\n",
        "Notice the importance of \"For\" and \"years\" in the prepositional phrase -- multiple tokens map to these two.\n",
        "\n",
        "Figure 1-6 displays the second part of the sentence parsed.\n",
        "\n",
        "![Dependency Parsing Example - Part 2](https://github.com/nlpbook/nlpbook/blob/main/images/hulp_0106.png?raw=1)\n",
        "\n",
        "The token \"was\" connects to the nominal subject \"Galileo\" and two prepositional phrases: \"under house arrest\" and \"for espousing this man's theory\".\n",
        "\n",
        "These figures show how certain tokens can be grouped together and how the groups of tokens are related to one another. This is an essential step in natural language processing. First, the machine breaks the sentence apart into tokens. Then it assigns metadata to each token (e.g., part of speech), and then it connects the tokens based on their relationship to one another.\n",
        "\n",
        "Let's move on to chunking, which is another form of grouping of related tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8PCF1eDd5-R"
      },
      "source": [
        "#### Chunking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxQemJtPd5-R"
      },
      "source": [
        "Let’s perform chunking on the following sentence: \"My parents live in New York City\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqgEzo1Hd5-R",
        "outputId": "9b91ec01-fd78-4c11-cda6-400709320246"
      },
      "source": [
        "# Print tokens for example sentence without chunking\n",
        "for token in nlp(\"My parents live in New York City.\"):\n",
        "    print(token.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "My\n",
            "parents\n",
            "live\n",
            "in\n",
            "New\n",
            "York\n",
            "City\n",
            ".\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2dkcEjVd5-S"
      },
      "source": [
        "Chunking combines related tokens into a single token.\n",
        "\n",
        "With chunking, the spaCy language model will identify \"My parents\" and \"New York City\" as noun chunks much like humans would when parsing a sentence in their head."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl6GF7ltd5-S",
        "outputId": "b580bd1e-8d2c-4ae4-dd36-8b69a74acb49"
      },
      "source": [
        "# Print chunks for example sentence\n",
        "for chunk in nlp(\"My parents live in New York City.\").noun_chunks:\n",
        "      print(chunk.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "My parents\n",
            "New York City\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qNs1TTWd5-S"
      },
      "source": [
        "By grouping related tokens into chunks, the machine will have an easier time processing the sentence. Instead of viewing each token in isolation, the machine now recognizes that certain tokens are related to others, a necessary step in natural language processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXiFtfekd5-S"
      },
      "source": [
        "#### Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdKg1EfPd5-S"
      },
      "source": [
        "Now, let’s go a step further and perform lemmatization. If you recall, lemmatization is the process to convert words into the base (or canonical) forms of the words. For example, horses to horse, slept to sleep, and biggest to big. Just like part-of-speech tagging, dependency parsing, and chunking, lemmatization helps the machine \"process\" the tokens. With lemmatization, the machine is able to simplify the tokens by converting some of the tokens into their most basic forms.\n",
        "\n",
        "Stemming is a related concept, but stemming is simpler. Stemming reduces words to their word stems, often using a rule-based approach.\n",
        "\n",
        "Lemmatization is a more difficult process but generally results in better outputs; stemming sometimes creates outputs that are non-sensical (non-words). In fact, spaCy does not even support stemming; it supports only lemmatization.\n",
        "\n",
        "We will create a DataFrame to store and view the original and lemmatized versions of tokens side-by-side."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rp_KCVHid5-T",
        "outputId": "d9e10195-7277-400d-b9e8-98ddb15d36c1"
      },
      "source": [
        "# Print Lemmatization for tokens in the first question\n",
        "lemmatization = pd.DataFrame(data=[], \\\n",
        "  columns=[\"original\",\"lemmatized\"])\n",
        "i = 0\n",
        "for token in example_question_tokens:\n",
        "    lemmatization.loc[i,\"original\"] = token.text\n",
        "    lemmatization.loc[i,\"lemmatized\"] = token.lemma_\n",
        "    i = i+1\n",
        "\n",
        "lemmatization"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original</th>\n",
              "      <th>lemmatized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>For</td>\n",
              "      <td>for</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>the</td>\n",
              "      <td>the</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>last</td>\n",
              "      <td>last</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>years</td>\n",
              "      <td>year</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>of</td>\n",
              "      <td>of</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>his</td>\n",
              "      <td>his</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>life</td>\n",
              "      <td>life</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>,</td>\n",
              "      <td>,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Galileo</td>\n",
              "      <td>Galileo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>was</td>\n",
              "      <td>be</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>under</td>\n",
              "      <td>under</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>house</td>\n",
              "      <td>house</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>arrest</td>\n",
              "      <td>arrest</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>for</td>\n",
              "      <td>for</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>espousing</td>\n",
              "      <td>espouse</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>this</td>\n",
              "      <td>this</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>man</td>\n",
              "      <td>man</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>'s</td>\n",
              "      <td>'s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>theory</td>\n",
              "      <td>theory</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     original lemmatized\n",
              "0         For        for\n",
              "1         the        the\n",
              "2        last       last\n",
              "3           8          8\n",
              "4       years       year\n",
              "5          of         of\n",
              "6         his        his\n",
              "7        life       life\n",
              "8           ,          ,\n",
              "9     Galileo    Galileo\n",
              "10        was         be\n",
              "11      under      under\n",
              "12      house      house\n",
              "13     arrest     arrest\n",
              "14        for        for\n",
              "15  espousing    espouse\n",
              "16       this       this\n",
              "17        man        man\n",
              "18         's         's\n",
              "19     theory     theory"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPxO4eIMd5-T"
      },
      "source": [
        "As you can see, words such as \"years\", \"was\", and \"espousing\" are lemmatized to their base forms. The other tokens are already their base forms, so the lemmatized output is the same as the original. Lemmatization simplifies tokens into their simplest forms, where possible, to simplify the process for the machine to parse sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIsooNT5d5-T"
      },
      "source": [
        "#### Named Entity Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvQdmAObd5-T"
      },
      "source": [
        "When combined together, everything we've done so far - tokenization, part-of-speech tagging, dependency parsing, chunking, and lemmatization - makes it possible for machines to perform more complex NLP tasks.\n",
        "\n",
        "One example of a complex NLP task is named entity recognition (NER). Named entity recognition parses notable entities in natural language and labels them with their appropriate class label. For example, NER labels names of people with the label \"Person\" and names of cities with the label \"Location.\" \n",
        "\n",
        "NER is possible only because the machine is able to perform text classification using the metadata generated by the earlier NLP tasks we've covered. Without the metadata from the earlier NLP tasks, the machine would have a very difficult time performing NER because it would not have enough features to classify names of people as \"Person,\" names of cities as \"Location,\" etc.\n",
        "\n",
        "NER is a valuable NLP task because many organizations need to process lots and lots of documents in volume, and the simple act of labeling notable entities with the appropriate class label is a meaningful first step in analyzing the textual information, particularly for information retrieval tasks (e.g., finding information that you need as quickly as possible).\n",
        "\n",
        "These documents include contracts, leases, real estate purchase agreements, financial reports, news articles, etc. Before named entity recogniton, humans would have had to label such entities by hand (at many companies, they still do). Now, named entity recognition (also known as \"NER\") provides an algorithmic way to perform this task.\n",
        "\n",
        "SpaCy's NER model is able to label many types of notable entities (\"real-world objects\"). Figure 1-7 displays the current set of entity types the spaCy model is able to recognize.\n",
        "\n",
        "![spaCy NER Entity Types](https://github.com/nlpbook/nlpbook/blob/main/images/hulp_0107.png?raw=1)\n",
        "\n",
        "It's very important to note that NER is, at its very core, a classification model. Using the context of tokens around the token of interest, the NER model predicts the entity type of the token of interest. NER is a statistical model, and the corpus of data the model has trained on matters a lot. For better performance, developers of these models in enterprise will finetune the base NER models on their particular corpus of documents to achieve better performance versus the base NER model.\n",
        "\n",
        "Let's try the spaCy NER model. We will perform NER on the first sentence describing George Washington, the first president of the United States, from his [Wikipedia article](https://en.wikipedia.org/wiki/George_Washington).\n",
        "\n",
        "Here's the sentence: George Washington was an American political leader, military general, statesman, and Founding Father who served as the first president of the United States from 1789 to 1797.\n",
        "\n",
        "As you can see above, there are several real-world objects to recognize here including George Washington and the United States."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj_tE2Mud5-T",
        "outputId": "58edb4e8-cfb5-4123-fc4e-1b7c553efbe0"
      },
      "source": [
        "# Print NER results\n",
        "example_sentence = \"George Washington was an American political leader, \\\n",
        "military general, statesman, and Founding Father who served as the \\\n",
        "first president of the United States from 1789 to 1797.\\n\"\n",
        "\n",
        "print(example_sentence)\n",
        "\n",
        "print(\"Text Start End Label\")\n",
        "doc = nlp(example_sentence)\n",
        "for token in doc.ents:\n",
        "    print(token.text, token.start_char, token.end_char, token.label_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "George Washington was an American political leader, military general, statesman, and Founding Father who served as the first president of the United States from 1789 to 1797.\n",
            "\n",
            "Text Start End Label\n",
            "George Washington 0 17 PERSON\n",
            "American 25 33 NORP\n",
            "first 119 124 ORDINAL\n",
            "the United States 138 155 GPE\n",
            "1789 to 1797 161 173 DATE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wC1-ZHPkd5-U"
      },
      "source": [
        "\n",
        "There are four elements to the output. First, the text that comprises the entity; note that the text could be a single token or a set of token that makes up the entire entity. Second, the start position of the text in the sentence. Third, the end position of the text in the sentence. Fourth, the label of the entity.\n",
        "\n",
        "To make the value of NER even more apparent, let’s use spaCy’s built-in visualizer to visualize this sentence with the releveant entity labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4gMgVZtd5-U",
        "outputId": "56a59860-e20c-4921-8d70-0ec47c583f19"
      },
      "source": [
        "# Visualize NER results\n",
        "displacy.render(doc, style='ent', jupyter=True, options={'distance': 120})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    George Washington\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " was an \n",
              "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    American\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
              "</mark>\n",
              " political leader, military general, statesman, and Founding Father who served as the \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    first\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
              "</mark>\n",
              " president of \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    the United States\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " from \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    1789 to 1797\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ".</br></div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crLSgi8md5-U"
      },
      "source": [
        "As you can see in Figure 1-8, the spaCy NER model does a great job labeling the entities. \"George Washington\" is a person and the text starts at index 0 and ends at index 17. His nationality is \"American\". \"first\" is labeled as an ordinal number, \"the United States\" is a geopolitical entity, and \"1789 to 1797\" is a date.\n",
        "\n",
        "![Visualize NER Results](https://github.com/nlpbook/nlpbook/blob/main/images/hulp_0108.png?raw=1)\n",
        "\n",
        "The sentence is beautifully rendered with color-coded labels based on the entity type. This is a powerful and meaningful NLP task; you could see how doing this machine-driven labeling at scale without humans could add a lot of value to enterprises that work with a lot of textual data. Of course, to train such a model in the first place, you do need to have a lot of humans that annotate textual data. And you may need humans in the loop to deal with edge cases in production. You are never really human-free, but perhaps you could ultimately get to a process that is mostly human-free."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlV6wSFOd5-U"
      },
      "source": [
        "#### Named Entity Linking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RttZkHY0d5-U"
      },
      "source": [
        "Another complex yet very useful NLP task in enterprise is named entity linking (NEL). Named entity linking resolves a textual entity to a unique identifier in a knowledge base. In other words, NEL resolves the entity in your source text to a canonical version in a knowledge database. Let’s try to link all entities that are named persons to Google’s Knowledge Graph. We will make a Google Knowledge Graph API call to perform this named entity linking.footnote:[You will need your own [Google Knowledge Graph API key](https://developers.google.com/knowledge-graph) to perform this API call on your own machine. We will perform this using our own API key for illustrative purposes.]\n",
        "\n",
        "Here is the function to perform this API call."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXByM3vOd5-V"
      },
      "source": [
        "# Import libraries\n",
        "import requests\n",
        "\n",
        "# Define Google Knowledge Graph API Result function\n",
        "def returnGraphResult(query, key, entityType):\n",
        "    if entityType==\"PERSON\":\n",
        "        google = f\"https://kgsearch.googleapis.com/v1/entities:search\\\n",
        "         ?query={query}&key={key}\"\n",
        "        resp = requests.get(google)\n",
        "        url = resp.json()['itemListElement'][0]['result']\\\n",
        "         ['detailedDescription']['url']\n",
        "        description = resp.json()['itemListElement'][0]['result']\\\n",
        "         ['detailedDescription']['articleBody']\n",
        "        return url, description\n",
        "    else:\n",
        "        return \"no_match\", \"no_match\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcgNtEj-d5-V"
      },
      "source": [
        "Let’s perform entity linking on our George Washington example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y11dhgI7d5-V",
        "outputId": "96ef1e4f-6146-4d19-e7e1-061856478b85"
      },
      "source": [
        "# Print Wikipedia descriptions and urls for entities\n",
        "# You can un-comment this and run the code after you obtain your own Google Knowledge Graph API key\n",
        "'''\n",
        "for token in doc.ents:\n",
        "    url, description = returnGraphResult(token.text, key, token.label_)\n",
        "    print(token.text, token.label_, url, description)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfor token in doc.ents:\\n    url, description = returnGraphResult(token.text, key, token.label_)\\n    print(token.text, token.label_, url, description)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVVqYR04d5-V"
      },
      "source": [
        "Here is the output.\n",
        "\n",
        "- George Washington:: PERSON https://en.wikipedia.org/wiki/George_Washington George Washington was an American political leader, military general, statesman, and Founding Father, who also served as the first President of the United States from 1789 to 1797. \n",
        "- American:: NORP no_match no_match\n",
        "- first:: ORDINAL no_match no_match\n",
        "- the United States:: GPE no_match no_match\n",
        "- 1789 to 1797:: DATE no_match no_match\n",
        "\n",
        "As you can see, George Washing is a PERSON and is linked successfully to the \"George Washington\" Wikipedia url and description. The rest are not of entity type PERSON and are not linked. If desired, we could link the other named entities, such as the United States, to relevant Wikipedia articles, too.\n",
        "\n",
        "Named entity linking has many use cases in enterprise, especially since the need to link information to a taxonomy comes up over and over again (e.g., linking stock tickers, pharmaceutical drugs, publicly traded companies, consumer products, etc. to canonical versions in a taxonomy or knowledge base)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyKelPUnd5-V"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buut6ZDod5-W"
      },
      "source": [
        "In this chapter, we defined NLP and covered its origins, including some of the commercial applications that are popular in enterprise today. Then, we defined some basic NLP tasks and performed them using a very performant NLP library known as SpaCy. You should spend more time using SpaCy, including reviewing documentation that is available online, to hone what you have learned in this chapter.\n",
        "\n",
        "While the tasks we performed are very basic, when combined together, NLP tasks such as tokenization, part-of-speech tagging, dependency parsing, chunking, and lemmatization make it possible for machines to perform even more complex NLP tasks such as named entity recognition and entity linking. We hope our walkthrough of these tasks helped you build some intuition on just how machines are able to unpack and process natural language, demystifying some of the space.\n",
        "\n",
        "Today, most complex NLP applications do not require practitioners to perform these tasks manually; rather neural networks learn to perform these \"tasks\" on their own. In the next chapter, we will dive into some of the state of the art approaches using the Transformer architecture and large, pretrained language models from fastai and Hugging Face to show just how easy it is to get up and running with NLP today. Later in the book, we will return to the basics (which we just teased you with briefly in this chapter) and help you build more of your foundational knowledge of NLP."
      ]
    }
  ]
}