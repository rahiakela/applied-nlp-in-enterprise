{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01-transfer-learning-with-fastai.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNIm29Mg2SchnqNdrUQtcgD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/applied-nlp-in-enterprise/blob/main/02-transformers-and-transfer-learning/01_transfer_learning_with_fastai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCksgCQIHTH6"
      },
      "source": [
        "##Transformers and Transfer Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrEcQA18H9tm"
      },
      "source": [
        "One of the most important ideas to implement if you want to get deep learning working in the real world is transfer learning, which is the process of taking a model that has already been trained on another dataset and fine-tuning it to fit your new dataset. For example, if you're training a language model to generate compelling short stories in the style of Hemingway, you could fine-tune a model trained on a wide variety of books instead of training on just the text samples of Hemingway, of which there may not be many.\n",
        "\n",
        "A nice analogy in object-oriented programming is the concept of inheritance in classes.\n",
        "\n",
        "By training on the larger dataset, the model essentially inherits a large amount of extra knowledge, which it can use to perform better on the task you care about. From a practical standpoint, transfer learning helps you get better performing models faster since fine-tuning, if done correctly, is often computationally cheaper than training from scratch.\n",
        "\n",
        ">Assuming that the original dataset you're transferring *from* is much larger than the dataset you're using for fine-tuning. If your fine-tuning dataset is larger, perhaps you should be applying transfer learning the other way around! But in practice, it's very hard to natural language text datasets that are of comparable size to the ones used for pretraining.\n",
        "\n",
        "The other big advancement we'll discuss is the use of a new kind of model architecture called the transformer. Training transformers can be complicated and does not always work well without some fine-tuning. So, instead of traning it from scratch, we'll show you the pretraining technique on another architecure, and the use a popular pre-trained transformer to perform inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Mu6MwQDI8cW"
      },
      "source": [
        "##fastai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2bNqevsJDEC"
      },
      "source": [
        "We're going to fine-tune a language model and then transform it into a text classifier that categorizes text based on sentiment. We'll start with the simplest working implementation, and progressively train our network using the [ULMFit](https://arxiv.org/abs/1801.06146) technique.\n",
        "\n",
        "The dataset we're going to use here is the IMDB movie review datset. It's not very fun, but it's simple and small, which is what we want when starting off.\n",
        "\n",
        "`fastai` is more more than your standard deep learning library. It includes tools that help you solve the problem at hand end-to-end as fast as possible. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgQws_0wJV2n"
      },
      "source": [
        "##Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyeiJNEGK4dU"
      },
      "source": [
        "!pip install fastai==2.2.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ns-owJlfJXJv"
      },
      "source": [
        "from fastai.text.all import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5Y04CRZKuzV"
      },
      "source": [
        "One of those tools is a built-in set of common datasets that can be easily downloaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "IzXC9SxdKvez",
        "outputId": "05a1ec2d-a45f-4c10-f8be-f7d93d716d45"
      },
      "source": [
        "path = untar_data(URLs.IMDB)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnOoD6dpLZ0W"
      },
      "source": [
        "This particular instance of the IMDB dataset is organized just like ImageNet is (i.e. one directory per class). So in this case, the positive reviews are saved under `pos` and the negative reviews are saved under `neg`.\n",
        "\n",
        "We can set up set up our dataset and prepare for training by using the `TextDataLoaders.from_folder` method built into `fastai`. The only thing we need to specify is the name of the validation folder, which is \"test\" (and not the default \"valid\")."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "tTXV9pgoLaWq",
        "outputId": "315f098c-ca2c-4f68-8e50-76d319fd9ae1"
      },
      "source": [
        "dls = TextDataLoaders.from_folder(path, valid=\"test\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RreN4spzLwA7"
      },
      "source": [
        "Another useful method is `show_batch`, which lets us take a quick glimpse at our data to make sure everything looks OK."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "H06Ewt0BMCUr",
        "outputId": "023c055b-8355-4078-9ebe-0f66141247fd"
      },
      "source": [
        "dls.show_batch()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>xxbos xxmaj match 1 : xxmaj tag xxmaj team xxmaj table xxmaj match xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley vs xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley started things off with a xxmaj tag xxmaj team xxmaj table xxmaj match against xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit . xxmaj according to the rules of the match , both opponents have to go through tables in order to get the win . xxmaj benoit and xxmaj guerrero heated up early on by taking turns hammering first xxmaj spike and then xxmaj bubba xxmaj ray . a xxmaj german xxunk by xxmaj benoit to xxmaj bubba took the wind out of the xxmaj dudley brother . xxmaj spike tried to help his brother , but the referee restrained him while xxmaj benoit and xxmaj guerrero</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>xxbos xxmaj i 've rented and watched this movie for the 1st time on xxup dvd without reading any reviews about it . xxmaj so , after 15 minutes of watching xxmaj i 've noticed that something is wrong with this movie ; it 's xxup terrible ! i mean , in the trailers it looked scary and serious ! \\n\\n i think that xxmaj eli xxmaj roth ( mr . xxmaj director ) thought that if all the characters in this film were stupid , the movie would be funny … ( so stupid , it 's funny … ? xxup wrong ! ) xxmaj he should watch and learn from better horror - comedies such xxunk xxmaj night \" , \" the xxmaj lost xxmaj boys \" and \" the xxmaj return xxmaj of the xxmaj living xxmaj dead \" ! xxmaj those are funny ! \\n\\n \"</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>xxbos i felt duty bound to watch the 1983 xxmaj timothy xxmaj dalton / xxmaj zelah xxmaj clarke adaptation of \" jane xxmaj eyre , \" because xxmaj i 'd just written an article about the 2006 xxup bbc \" jane xxmaj eyre \" for xxunk . \\n\\n xxmaj so , i approached watching this the way xxmaj i 'd approach doing homework . \\n\\n i was irritated at first . xxmaj the lighting in this version is bad . xxmaj everyone / everything is washed out in a bright white xxunk light that , in some scenes , casts shadows on the wall behind the characters . \\n\\n xxmaj and the sound is poorly recorded . i felt like i was listening to a high school play . \\n\\n xxmaj and the pancake make - up is way too heavy . \\n\\n xxmaj and the sets do n't fully</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>xxbos xxmaj to be a xxmaj buster xxmaj keaton fan is to have your heart broken on a regular basis . xxmaj most of us first encounter xxmaj keaton in one of the brilliant feature films from his great period of independent production : ' the xxmaj general ' , ' the xxmaj navigator ' , ' sherlock xxmaj jnr ' . xxmaj we recognise him as the greatest figure in the entire history of film comedy , and we want to see more of his movies . xxmaj here the heartbreak begins . xxmaj after ' steamboat xxmaj bill xxmaj jnr ' , xxmaj keaton 's brother - in - law xxmaj joseph xxmaj xxunk pressured him into signing a contract that put xxmaj keaton under the control of xxup mgm . xxmaj keaton became just one more actor for hire , performing someone else 's scripts . xxmaj</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>xxbos i have n't liked many xxup tv shows post 1990 , but xxup that 70s xxup show is great . xxmaj never seeing it during it 's first run , thinking a gimmicky period piece , i was wrong ! i started watching in reruns and the more i watched , the more i liked ! xxmaj now , it is the only show xxunk xxunk that i watch regularly . \\n\\n xxmaj although xxup that 70s xxup show mimics some of the styles , attitudes , music , and tastes of the 70s , it does not mire itself in that decade by going overboard with the references and look of the 70s . xxmaj it contains so much funny , witty , biting dialogue that is delivered with confidence and certainty by its main cast that it overcomes any 70s clichés by just being humorous . xxmaj</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>xxbos xxmaj office work , especially in this era of computers , multi - functional copy machines , e - mail , voice mail , snail mail and ` temps , ' is territory ripe with satirical possibilities , a vein previously tapped in such films as ` clockwatchers ' and ` office xxmaj space , ' and very successfully . xxmaj this latest addition to the temp / humor pool , however , ` haiku xxmaj tunnel , ' directed by xxmaj josh xxmaj kornbluth and xxmaj jacob xxmaj kornbluth , fails to live up to it 's predecessors , and leaves the laughs somewhere outside the door , waiting for a chance to sneak in . xxmaj unfortunately for the audience , that chance never comes ; so what you get is a nice try , but as the man once said , no cigar . \\n\\n\\t xxmaj</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>xxbos xxmaj this film reminds me of 42nd xxmaj street starring xxmaj bebe xxmaj daniels and xxmaj ruby xxmaj keeler . xxmaj when i watch this film a lot of it reminded me of 42nd xxmaj street , especially the character xxmaj eloise who 's a temperamental star and she ends up falling and breaks her ankle , like xxmaj bebe xxmaj daniels did in 42nd xxmaj street and another performer gets the part and become a star . xxmaj this film , like most race films , keeps people watching because of the great entertainment . xxmaj race films always showed xxmaj black xxmaj entertainment as it truly was that was popular in that time era . xxmaj the xxmaj dancing xxmaj styles , xxmaj the xxmaj music , xxmaj dressing xxmaj styles , xxmaj you 'll xxmaj love xxmaj it . xxmaj this movie could of been big</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>xxbos xxmaj an xxmaj american xxmaj in xxmaj paris is an integrated musical , meaning that the songs and dances blend perfectly with the story . xxmaj the film was inspired by the 1928 orchestral composition by xxmaj george xxmaj gershwin . \\n\\n xxmaj the story of the film is interspersed with show - stopping dance numbers choreographed by xxmaj gene xxmaj kelly and set to popular xxmaj gershwin tunes . xxmaj songs and music include \" i xxmaj got xxmaj rhythm , \" \" 's xxmaj wonderful , \" and \" our xxmaj love is xxmaj here to xxmaj stay \" . xxmaj it set a new standard for the subgenre known as the \" songbook \" musical with dozens of xxmaj gershwin tunes buried in the underscore . xxmaj the climax is \" the xxmaj american in xxmaj paris \" ballet , an 18 minute dance featuring xxmaj</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>xxbos xxmaj streisand fans only familiar with her work from the xxup funny xxup girl film onwards need to see this show to see what a brilliant performer xxmaj streisand xxup was - xxup before she achieved her goal of becoming a xxmaj movie xxmaj star . xxmaj there had never been a female singer quite like her ever before , and there never would be again ( sorry , xxmaj celine - only in your dreams ! ) , but never again would xxmaj streisand sing with the vibrancy , energy , and , above all , the xxup enthusiasm and xxup vulnerability with which she performs here - by the time she gets to that xxmaj central xxmaj park concert only 2 or 3 years later , she 'd been filming xxup funny xxup girl in xxmaj hollywood and her performing style has become less spontaneous and more</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJK5r60Rf1V7"
      },
      "source": [
        "We can see that the library automatically processed all the texts to split then in *tokens*, adding some special tokens like:\n",
        "\n",
        "- `xxbos` to indicate the beginning of a text\n",
        "- `xxmaj` to indicate the next word was capitalized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cSG-EvbyGNK"
      },
      "source": [
        "## fastai Learner for text classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4Qk5DfhyJuR"
      },
      "source": [
        "`fastai` uses an object called a `Learner` for doing pretty much everything. We can construct one for text classification in one line of code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "0S5LR5tlgFHk",
        "outputId": "12cb95fa-cd68-43c1-a237-daef0ca7fb01"
      },
      "source": [
        "learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGsbUF8Vgfmz"
      },
      "source": [
        "Instead of the transformer model that we've been raving about (and will continue to dicuss) throughout a vast majority of the book, we're going to use the [AWD LSTM](https://arxiv.org/abs/1708.02182) architecture instead for now, since it's easier and faster to train.\n",
        "\n",
        "There are a few other details: `drop_mult` is a parameter that controls the magnitude of all dropouts in that model, and we use `accuracy` to track down how well we are doing.\n",
        "\n",
        "With the `Learner` defined, we can now fine-tune our pretrained model, using a method with an unsurprising name:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "UHECuT4Vgjs9",
        "outputId": "85cd8944-508e-4e78-ae1f-bd52d10cc997"
      },
      "source": [
        "learn.fine_tune(4, 1e-2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.458047</td>\n",
              "      <td>0.412425</td>\n",
              "      <td>0.813520</td>\n",
              "      <td>03:26</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.307921</td>\n",
              "      <td>0.297660</td>\n",
              "      <td>0.877520</td>\n",
              "      <td>07:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.247349</td>\n",
              "      <td>0.202260</td>\n",
              "      <td>0.920680</td>\n",
              "      <td>07:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.192098</td>\n",
              "      <td>0.191875</td>\n",
              "      <td>0.926680</td>\n",
              "      <td>07:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.146141</td>\n",
              "      <td>0.192024</td>\n",
              "      <td>0.929520</td>\n",
              "      <td>07:01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wc1imdAKg6fz"
      },
      "source": [
        "93% accuracy look good! But let's see how well it's actually doing..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lVLj4mo8g7Av",
        "outputId": "c0b2d19d-9b15-40be-85fc-09308ef0fb8d"
      },
      "source": [
        "learn.show_results()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "      <th>category_</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>xxbos xxmaj there 's a sign on xxmaj the xxmaj lost xxmaj highway that says : \\n\\n * major xxup spoilers xxup ahead * \\n\\n ( but you already knew that , did n't you ? ) \\n\\n xxmaj since there 's a great deal of people that apparently did not get the point of this movie , xxmaj i 'd like to contribute my interpretation of why the plot makes perfect sense . xxmaj as others have pointed out , one single viewing of this movie is not sufficient . xxmaj if you have the xxup dvd of xxup md , you can \" cheat \" by looking at xxmaj david xxmaj lynch 's \" top 10 xxmaj hints to xxmaj unlocking xxup md \" ( but only upon second or third viewing , please . ) ;) \\n\\n xxmaj first of all , xxmaj mulholland xxmaj drive is</td>\n",
              "      <td>pos</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>xxbos i really wanted to be able to give this film a 10 . xxmaj i 've long thought it was my favorite of the four modern live - action xxmaj batman films to date ( and maybe it still will be -- i have yet to watch the xxmaj schumacher films again ) . xxmaj i 'm also starting to become concerned about whether xxmaj i 'm somehow subconsciously being contrarian . xxmaj you see , i always liked the xxmaj schumacher films . xxmaj as far as i can remember , they were either 9s or 10s to me . xxmaj but the conventional wisdom is that the two xxmaj tim xxmaj burton directed films are far superior . i had serious problems with the first xxmaj burton xxmaj batman this time around -- i ended up giving it a 7 - -and apologize as i might ,</td>\n",
              "      <td>pos</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>xxbos \" buffalo xxmaj bill , xxmaj hero of the xxmaj far xxmaj west \" director xxmaj mario xxmaj costa 's unsavory xxmaj spaghetti western \" the xxmaj beast \" with xxmaj klaus xxmaj kinski could only have been produced in xxmaj europe . xxmaj hollywood would never dared to have made a western about a sexual predator on the prowl as the protagonist of a movie . xxmaj never mind that xxmaj kinski is ideally suited to the role of ' crazy ' xxmaj johnny . xxmaj he plays an individual entirely without sympathy who is ironically dressed from head to toe in a white suit , pants , and hat . xxmaj this low - budget oater has nothing appetizing about it . xxmaj the typically breathtaking xxmaj spanish scenery around xxmaj almeria is nowhere in evidence . xxmaj instead , xxmaj costa and his director of photography</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>xxbos xxmaj this is , per se , an above average film but why in the name of xxmaj bog was it made ? xxmaj it 's impossible to treat it as a thing unto itself because it is an almost shot - for - shot remake of an xxmaj alfred xxmaj hitchcock classic of 1960 . xxmaj you ca n't watch it without the 1960 film nudging into your consciousness . \\n\\n xxmaj what does the word \" credit \" mean ? xxmaj how can we credit xxmaj van xxmaj xxunk and his associates with anything except deciding to use different actors , slightly different sets , and color ? \\n\\n xxmaj anne xxmaj heche is attractive but lacks xxmaj janet xxmaj leigh 's stolid determination to become a respectable middle - class woman . xxmaj and xxmaj heche is younger than xxmaj leigh , who brought to her</td>\n",
              "      <td>neg</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>xxbos xxmaj this is one of those films where it is easy to see how some people would n't like it . xxmaj my wife has never seen it , and when i just rewatched it last night , i waited until after she went to bed . xxmaj she might have been amused by a couple small snippets , but i know she would have had enough within ten minutes . \\n\\n xxmaj head has nothing like a conventional story . xxmaj the film is firmly mired in the psychedelic era . xxmaj it could be seen as filmic surrealism in a nutshell , or as something of a postmodern acid trip through film genres . xxmaj if you 're not a big fan of those things -- psychedelia , surrealism , postmodernism and the \" acid trip aesthetic \" ( assuming there 's a difference between them )</td>\n",
              "      <td>pos</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>xxbos xxmaj clayton xxmaj moore made his last official appearance on screen as the xxmaj masked xxmaj man in director xxmaj lesley xxmaj selander 's epic adventure \" the xxmaj lone xxmaj ranger and the xxmaj lost xxmaj city of xxmaj gold , \" co - starring xxmaj jay xxmaj silverheels as his faithful xxmaj indian scout xxmaj tonto . xxmaj selander was an old hand at helming westerns during his 40 years in films and television with over a 100 westerns to his directorial credit . xxmaj this fast - paced horse opera embraced a revisionist perspective in its depiction of xxmaj native xxmaj americans that had been gradually gaining acceptance since 1950 in xxmaj hollywood oaters after director xxmaj delmar xxmaj daves blazed the trail with the xxmaj james xxmaj stewart western \" broken xxmaj arrow . \" xxmaj racial intolerance figures as the primary theme in the</td>\n",
              "      <td>pos</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>xxbos \" stripperella \" is an animated series about a girl named xxmaj erotica xxmaj jones ( voiced by xxmaj pamela xxmaj anderson ) who lives a double life as a stripper at a gentleman 's club known as \" the xxmaj tender xxmaj loins \" and as a sexy crime - fighter known as xxmaj stripperella , a.k.a . xxmaj agent 69 who works for a government organization . xxmaj as xxmaj stripperella , xxmaj erotica fights crime and the forces of evil such as a plastic surgeon who gives women breast implants that either explode or make them fat and xxmaj cheapo , a criminal who steals from 99 cent stores and makes his two henchmen share a gun . xxmaj the creator of the character and the series is xxmaj stan xxmaj lee of xxmaj marvel fame ( and creator of spider - man ) . \\n\\n</td>\n",
              "      <td>pos</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>xxbos xxmaj in a world in which debatable and misunderstood subjects can be listed endlessly , this powerful 1995 film takes on one at the top of that list ; moreover , it does it objectively and realistically , and with a sensibility and sensitivity that makes it a truly great film by anyone 's measuring stick . xxmaj and to add some irony to it all , even the subject matter of this film has been widely misunderstood , as it is wrongly perceived that this is a film about the pros and cons of the death penalty ; it is not . xxmaj at the heart of ` dead xxmaj man xxmaj walking , ' directed by xxmaj tim xxmaj robbins , is a subject that in reality is possibly the most misunderstood of all , and with good reason , because it just may be the hardest</td>\n",
              "      <td>pos</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>xxbos a space ship cruising through the galaxy encounters a mysterious cargo ship apparently adrift in space . xxmaj the crew investigates , hoping to lay claim to its cargo and acquire the ship . xxmaj however , once aboard the ominous vessel , their own ship mysteriously xxunk , leaving them to fend for themselves and battle none other then xxmaj count xxmaj dracula or xxmaj orloff as this creature calls himself . \\n\\n xxmaj not a bad start . i mean it follows any number of typical sci - fi / horror plots . xxmaj the genres have been around enough that even the most original story will inevitably invoke comparison to some other film . xxmaj but , when you start with a fairly typical horror convention , the legend of xxmaj dracula and vampires in general , and combine it with a fairly typical sci -</td>\n",
              "      <td>neg</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLbui1i3hL3N"
      },
      "source": [
        "We can also run prediction on individual sentences one at a time:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "j3cNyUrOhMVZ",
        "outputId": "ceefb6ba-880e-4ed2-98c5-f28d3ef66fa4"
      },
      "source": [
        "learn.predict(\"That movie was wicked cool!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('pos', tensor(1), tensor([0.2645, 0.7355]))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jwayt8byhao0"
      },
      "source": [
        "Our model predicts that the review is positive, as expected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tTmrII_LwJV"
      },
      "source": [
        "##ULMFiT for Transfer Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tha3nn8MLxZ5"
      },
      "source": [
        "The pretrained model we used in the previous section is called a language model. It was trained to guess the next word on a set of Wikipedia articles after reading all the words before. We got great results by directly fine-tuning this language model to a movie review classifier, but with one extra step, we can do even better.\n",
        "\n",
        "The Wikipedia English is slightly different from the IMDb English. So instead of jumping directly to the classifier, we could fine-tune our pretrained language model to the IMDb dataset and then use *that* as the base for our classifier instead of the Wikipedia language model.\n",
        "\n",
        "But beyond that, another very important reason this is useful is because we often have more data for our than we have *labelled* data. Labelling is expensive and generally requires human time and effort, so it's not uncommon to have a large database of text record where only a small subset of them are used for say, document tagging. But with this fine-tuning approach, we can still use the unlabelled data to fine-tune the *language model* even before we train the \n",
        "\n",
        "At the risk of dragging on a flawed analogy, this is almost like getting access to years of previous SAT passages. None of them will show up on the test *exactly*, but practicing them will help get a sense of what the SAT is like.\n",
        "\n",
        "This approach is called ULMFiT, introducted by Jeremy Howard and Sebastian Ruder in 2018. The process is summarized in below.\n",
        "\n",
        "<img src='https://github.com/rahiakela/applied-nlp-in-enterprise/blob/main/02-transformers-and-transfer-learning/images/ulmfit.png?raw=1' width='800'/>\n",
        "\n",
        "Since we already have the pretrained Wikipedia language model, we can start with step 2 of the pipeline - fine-tuning the IMDB language model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Asq-uQey1X1S"
      },
      "source": [
        "## Fine-Tuning a Language Model on IMDB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNgC-l-s1Y9w"
      },
      "source": [
        "We can get our texts in a `DataLoaders` suitable for language modeling very easily:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TC7EsqXh1li1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "deae6415-6a58-4d78-de33-5394f16de8ed"
      },
      "source": [
        "dls_lm = TextDataLoaders.from_folder(path, is_lm=True, valid_pct=0.1)\n",
        "dls_lm.show_batch(max_n=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>text_</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>xxbos xxmaj this movie can be interpreted on many different levels . xxmaj do n't listen to the other comments bashing the movie and saying that it is a played again story or w / e and that it is just about drugs . xxmaj it has very overt superficial metaphors about drugs ; however , the rest of the movie ( and why i think it was personally made ) is</td>\n",
              "      <td>xxmaj this movie can be interpreted on many different levels . xxmaj do n't listen to the other comments bashing the movie and saying that it is a played again story or w / e and that it is just about drugs . xxmaj it has very overt superficial metaphors about drugs ; however , the rest of the movie ( and why i think it was personally made ) is really</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>point , xxmaj nostradamus is in a carriage , having visions of xxup wwii , shouting ' stop xxmaj stop ! ' over and over . i wonder if the real xxmaj nostradamus could indeed see the future , and if so , could have foreseen this horrible film and been suitably upset by it . xxmaj perhaps he was really shouting at the director and writer in this scene . xxmaj</td>\n",
              "      <td>, xxmaj nostradamus is in a carriage , having visions of xxup wwii , shouting ' stop xxmaj stop ! ' over and over . i wonder if the real xxmaj nostradamus could indeed see the future , and if so , could have foreseen this horrible film and been suitably upset by it . xxmaj perhaps he was really shouting at the director and writer in this scene . xxmaj there</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>it back on the air for fans of the show to see . xxbos i ca n't say i liked nothing about all three episodes , but there was a lot not to like . \\n\\n xxmaj the main drag here is that i spent most of xxunk and all of xxunk being constantly reminded i was watching a show here and how clever it all was . xxmaj to answer the</td>\n",
              "      <td>back on the air for fans of the show to see . xxbos i ca n't say i liked nothing about all three episodes , but there was a lot not to like . \\n\\n xxmaj the main drag here is that i spent most of xxunk and all of xxunk being constantly reminded i was watching a show here and how clever it all was . xxmaj to answer the above</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>from either camera equipment or other sources in the kitchen . \\n\\n xxmaj all these xxunk in the first 5 minutes or so . xxmaj need i say more ? \\n\\n a good school project or fun project for friends to watch , but should never have been released for a real audience , especially not for a xxup paying audience . xxup this xxup was a xxup rip xxup off unless</td>\n",
              "      <td>either camera equipment or other sources in the kitchen . \\n\\n xxmaj all these xxunk in the first 5 minutes or so . xxmaj need i say more ? \\n\\n a good school project or fun project for friends to watch , but should never have been released for a real audience , especially not for a xxup paying audience . xxup this xxup was a xxup rip xxup off unless you</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>a few minutes , her ghost appears as a very vengeful spirit . xxmaj carnage of course ensues , and also some extremely gory killings , and also , some extremely sexy scenes . xxmaj what more could you ask for , you might ask ? xxmaj well , not much , because this baby has it all , and a very satirical ending , that should leave a smile on most</td>\n",
              "      <td>few minutes , her ghost appears as a very vengeful spirit . xxmaj carnage of course ensues , and also some extremely gory killings , and also , some extremely sexy scenes . xxmaj what more could you ask for , you might ask ? xxmaj well , not much , because this baby has it all , and a very satirical ending , that should leave a smile on most viewers</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6NvQrPV3WjQ"
      },
      "source": [
        "We need to pass something for `valid_pct` otherwise this method will try to split the data by using the grandparent folder names. By passing `valid_pct=0.1`, we tell it to get a random 10% of those reviews for the validation set.\n",
        "\n",
        "Then we have a convenience method to directly grab a `Learner` from it, using the `AWD_LSTM` architecture like before. We use accuracy and perplexity as metrics (the later is the exponential of the loss) and we set a default weight decay of 0.1. `to_fp16` puts the `Learner` in mixed precision, which is going to help speed up training on GPUs that have Tensor Cores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEqixYzP4DhJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "e6fe7a0e-560c-48f4-c511-6490993da8fd"
      },
      "source": [
        "learn = language_model_learner(dls_lm,\n",
        "                               AWD_LSTM,\n",
        "                               metrics=[accuracy, Perplexity()],\n",
        "                               path=path,\n",
        "                               wd=0.1).to_fp16()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKzXZTQs4zLr"
      },
      "source": [
        "By default, a pretrained `Learner` is in a frozen state, meaning that only the head of the model will train while the body stays frozen. \n",
        "\n",
        "We show you what is behind the fine_tune method here and use a fit_one_cycle method to fit the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "198965hG40UY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "d0cc0777-5bc2-4e1e-fb7e-c974ee1b1f8c"
      },
      "source": [
        "learn.fit_one_cycle(1, 1e-2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>perplexity</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>4.417480</td>\n",
              "      <td>4.118844</td>\n",
              "      <td>0.285167</td>\n",
              "      <td>61.488121</td>\n",
              "      <td>2:11:14</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiVJ9Gb35Bec"
      },
      "source": [
        "This model takes a while to train, so it's a good opportunity to talk about saving intermediary results. \n",
        "\n",
        "You can easily save the state of your model like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bbnqt7I5EDt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "738c77fd-367b-431c-8188-4d2c0ac00cc0"
      },
      "source": [
        "learn.save(\"1epoch\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Path('/root/.fastai/data/imdb/models/1epoch.pth')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7JW4Lsv5Hmn"
      },
      "source": [
        "It will create a file in `learn.path/models/` named \"1epoch.pth\". \n",
        "\n",
        "If you want to load your model on another machine after creating your `Learner` the same way, or resume training later, you can load the content of this file with:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmT4oYAf5bUX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a3e5186-1b5b-4b62-a8ea-b6f5876d82fe"
      },
      "source": [
        "learn.load(\"1epoch\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<fastai.text.learner.LMLearner at 0x7ff9ab4dbe50>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yBsFPz15sCy"
      },
      "source": [
        "We can then fine-tune the model after unfreezing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBhMpDn95slC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "cf91fc62-9b10-4dd7-d4c0-28e8160689a2"
      },
      "source": [
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(10, 1e-3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='1' class='' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      10.00% [1/10 2:24:16<21:38:26]\n",
              "    </div>\n",
              "    \n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>perplexity</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>4.095802</td>\n",
              "      <td>3.926311</td>\n",
              "      <td>0.303650</td>\n",
              "      <td>50.719517</td>\n",
              "      <td>2:24:16</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='4312' class='' max='5842' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      73.81% [4312/5842 1:42:36<36:24 4.0601]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='1' class='' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      10.00% [1/10 2:24:16<21:38:26]\n",
              "    </div>\n",
              "    \n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>perplexity</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>4.095802</td>\n",
              "      <td>3.926311</td>\n",
              "      <td>0.303650</td>\n",
              "      <td>50.719517</td>\n",
              "      <td>2:24:16</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='5606' class='' max='5842' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      95.96% [5606/5842 2:13:14<05:36 4.0350]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOoMzqK17dxq"
      },
      "source": [
        "Once this is done, we save all of our model except the final layer that converts activations to probabilities of picking each token in our vocabulary. The model not including the final layer is called the *encoder*. \n",
        "\n",
        "We can save it with `save_encoder`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNjcq44J7f02"
      },
      "source": [
        "learn.save_encoder(\"finetuned\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKjax7QF7mrV"
      },
      "source": [
        "Before using this to fine-tune a classifier on the reviews, we can use our model to generate random reviews: \n",
        "\n",
        "since it's trained to guess what the next word of the sentence is, we can use it to write new reviews:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAB52RYc7sww"
      },
      "source": [
        "TEXT = \"I liked this movie because\"\n",
        "N_WORDS = 40\n",
        "N_SENTENCES = 2\n",
        "\n",
        "preds = [learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)]\n",
        "print(\"\\n\".join(preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QK0n1lH58G5x"
      },
      "source": [
        "With the language model fine-tuned on movie review, we can now modify it to **classify** movie reviews. \n",
        "\n",
        "**The idea is that at this point, if the model is \"smart enough\" to predict the next word, it *must* be able to a simple positive/negative classification.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fG47X6va8MB9"
      },
      "source": [
        "##Training a text classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdAG_vrD8QdM"
      },
      "source": [
        "Using the same method as before, we can load the IMDb dataset again, but this time, we’ll be using it for text classification. Note that we pass in the vocabulary as a parameter.\n",
        "\n",
        "This is to make sure that the text classifier understands the same set of words\n",
        "that the language model was trained on:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7EVo9xMLvsh"
      },
      "source": [
        "dls_classifier = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid=\"test\", text_vocab=dls_lm.vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfZmFGQCMhZZ"
      },
      "source": [
        "Then we can define our text classifier like before:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EKo5L3VMhzN"
      },
      "source": [
        "learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_G_E8Fm1Mrkx"
      },
      "source": [
        "The difference is that before training it, we load the previous encoder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XA7Y_8pJMuPh"
      },
      "source": [
        "learn = learn.load_encoder(\"finetuned\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTRKL3weM8Kh"
      },
      "source": [
        "The last step is to train with discriminative learning rates and gradual unfreezing. \n",
        "\n",
        "In computer vision, we often unfreeze the model all at once, but for NLP classifiers, we find that unfreezing a few layers at a time makes a real difference:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6WRu7LbM-7J"
      },
      "source": [
        "learn.fit_one_cycle(1, 2e-2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJdlp-5_Oaae"
      },
      "source": [
        "In just one epoch we get the same result as our training in the first section—not too bad! We can pass -2 to `freeze_to` to freeze all except the last two parameter groups:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNoJ_fvPOoaZ"
      },
      "source": [
        "learn.freeze_to(-2)\n",
        "learn.fit_one_cycle(1, slice(1e-2 / (2.6 ** 4), 1e-2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJIII7GOO9sB"
      },
      "source": [
        "Then we can unfreeze a bit more, and continue training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0_WZ6nLO-Gs"
      },
      "source": [
        "learn.freeze_to(-3)\n",
        "learn.fit_one_cycle(1, slice(5e-3 / (2.6 ** 4), 5e-3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtIq4oQwPRsS"
      },
      "source": [
        "Finally, we can unfreeze the entire model, and let it train all the layers to get a final boost in accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8V0FqPWPS5_"
      },
      "source": [
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(2, slice(1e-3 / (2.6 ** 4), 1e-3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdlmJYHrV2ep"
      },
      "source": [
        "Now, you have a text classification model that can accurately predict if a movie review has positive or negative sentiment based on the raw text content of the review alone.\n",
        "\n",
        "With an understanding of the fastai APIs, you should now be able to implement\n",
        "your own text classifier on a dataset of your choice.\n",
        "\n",
        "While the IMDb dataset was fairly simple, many real-world NLP problems today can\n",
        "be formulated as text classification problems. Some of the things you can do with text classification include:\n",
        "\n",
        "- Predicting the programming language of some source code\n",
        "- Building a simple email spam classifier\n",
        "- Improving the functionality of an automated content moderation bot for online\n",
        "chats or forums\n",
        "- Categorizing documents based on their language\n",
        "\n"
      ]
    }
  ]
}